{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a43364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########import packages##########\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from tensorflow.keras.backend.tensorflow_backend import set_session\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense \n",
    "from tensorflow.keras.layers import Dropout \n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier \n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "#from tensorflow.keras.constraints import maxnorm \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "%matplotlib\n",
    "\n",
    "###########fix random seed for reproducability##########\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "'''\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "gpu_options =tf.GPUOptions(per_process_gpu_memory_fraction=0.8,allow_growth=True) ##每个gpu占用0.8存\n",
    "config=tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=True)\n",
    "sess=tf.Session(config=config)\n",
    "'''\n",
    "seed=1\n",
    "np.random.seed(seed)\n",
    "###########wrapping root mean square error for later calls##########\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "###########loading data##########\n",
    "csv = \"../../database/database422.csv\"\n",
    "file = open('../../results/ANN_Opt_parm/parameter.txt','w')\n",
    "\n",
    "fdata=pd.read_csv(csv,encoding=\"gbk\", index_col=0)\n",
    "raw_data = fdata.iloc[:, :]\n",
    "###########handling missing values##########\n",
    "median_raw_data=raw_data.median()\n",
    "dict_median_raw_data=median_raw_data.to_dict()\n",
    "data=raw_data.fillna(dict_median_raw_data)\n",
    "###########data standardization##########\n",
    "standardized_data = (data-np.mean(data,axis=0))/np.std(data,axis=0)#即简单实现标准化\n",
    "###########train test splitting##########\n",
    "param=standardized_data.iloc[:,0:422]\n",
    "power=standardized_data.iloc[:,422]\n",
    "X=param.values.astype(np.float32)\n",
    "y=power.values.astype(np.float32)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.15, random_state=751)\n",
    "\n",
    "###########search neuron network hyperparmeter space##########\n",
    "neurons=750\n",
    "max_corr_ann=0\n",
    "for activation in ['relu','tanh','sigmoid','softsign']:\n",
    "    for regularizer_term in [0.0001,0.0005,0.001,0.005,0.01,0.05,0.1]:\n",
    "        for dropout in [0.1,0.5]:\n",
    "            for epochs_number in range(1000,1200):\n",
    "                for batch_size_number in range (20,200,40):\n",
    "                    for learning_rate_search in [0.001,0.005,0.01,0.05]:\n",
    "                        ###########implementing hyperparameters##########\n",
    "                        neurons1=neurons\n",
    "                        activation1=activation\n",
    "                        regularizer=keras.regularizers.l2(regularizer_term)\n",
    "                        dropout_rate=dropout\n",
    "                        ###########keras ANN model construction##########\n",
    "                        model = Sequential() \n",
    "                        model.add(Dense(neurons1, input_dim=422, kernel_initializer='random_normal',\n",
    "                                        bias_initializer='random_normal',activation=activation1,kernel_regularizer=regularizer)) \n",
    "                        model.add(Dropout(dropout_rate))\n",
    "                        model.add(Dense(neurons1, input_dim=neurons1, kernel_initializer='random_normal',\n",
    "                                    bias_initializer='random_normal',activation=activation1,kernel_regularizer=regularizer)) \n",
    "                        model.add(Dropout(dropout_rate)) \n",
    "                        model.add(Dense(1, input_dim=neurons1, activation='linear'))\n",
    "                        adam=optimizers.Adam(lr=learning_rate_search)\n",
    "                        model.compile(loss='mse', optimizer=adam) \n",
    "                        #print('Training ------------')\n",
    "                        ###########train the model with the training set##########\n",
    "                        ###########testset has been remove before##########\n",
    "                        model.fit(X_train, y_train,verbose=0, epochs=epochs_number, batch_size=batch_size_number,validation_split=0.1,callbacks=[TensorBoard(log_dir='mytensorboard')])\n",
    "                        loss= model.evaluate(X_test, y_test)\n",
    "                        predict_ann= model.predict(X_test)\n",
    "                        train_ann= model.predict(X_train)\n",
    "                        ###########result output##########\n",
    "                        x_prediction_maximum_power_ann=predict_ann*np.std(data,axis=0)[422]+np.mean(data,axis=0)[422]\n",
    "                        y_real_maximum_power=y_test*np.std(data,axis=0)[422]+np.mean(data,axis=0)[422]\n",
    "                        x_prediction_maximum_power_ann=x_prediction_maximum_power_ann[:,0]\n",
    "                        x_prediction_maximum_power_ann_series=pd.Series(x_prediction_maximum_power_ann)\n",
    "                        y_real_maximum_power_series=pd.Series(y_real_maximum_power)\n",
    "                        corr_ann = round(x_prediction_maximum_power_ann_series.corr(y_real_maximum_power_series), 4)\n",
    "                        rmse_val= rmse(x_prediction_maximum_power_ann,y_real_maximum_power)\n",
    "                        print('ANN,R2',corr_ann,'RMSE',rmse_val)\n",
    "                        if corr_ann>0.85:\n",
    "                            mm1 ='%f'%corr_ann\n",
    "                            mw1 ='%f'%rmse_val\n",
    "                            neurons1 = '%d'%neurons\n",
    "                            regularizer_term1='%f'%regularizer_term\n",
    "                            dropout1='%f'%dropout\n",
    "                            epochs_number1='%f'%epochs_number\n",
    "                            batch_size_number1='%f'%batch_size_number\n",
    "                            file.write('ANN,R2:'+' '+ mm1 +' '+ 'RMSE:'+' '+mw1+ '\\n')\n",
    "                            file.write('neurons:'+' '+ neurons1 +' '+ 'regularizer_term:'+' '+regularizer_term1+ '\\n')\n",
    "                            file.write('dropout:'+' '+ dropout1 +' '+ 'batch_size_number:'+' '+batch_size_number1+ '\\n')\n",
    "                            if corr_ann > max_corr_ann:\n",
    "                                max_corr_ann = corr_ann\n",
    "                                max_rmse_val = rmse_val\n",
    "                                max_y_real_maximum_power = y_real_maximum_power\n",
    "                                max_x_prediction_maximum_power_ann = x_prediction_maximum_power_ann\n",
    "                                max_learning_rate_search = learning_rate_search\n",
    "                                max_neurons = neurons\n",
    "                                max_activation = activation\n",
    "                                max_regularizer_term = regularizer_term\n",
    "                                max_dropout = dropout\n",
    "                                max_epochs_number = epochs_number\n",
    "                                max_batch_size_number = batch_size_number\n",
    "                        if 0.9<corr_ann<1:\n",
    "                            break\n",
    "                        else:\n",
    "                            K.clear_session()\n",
    "                    else:continue\n",
    "                    break\n",
    "                else:continue\n",
    "                break\n",
    "            else:continue\n",
    "            break\n",
    "        else:continue\n",
    "        break\n",
    "    else:continue\n",
    "    break\n",
    "\n",
    "###########print best hyperparameter##########\n",
    "print(learning_rate_search)\n",
    "print(neurons)\n",
    "print(activation)\n",
    "print(regularizer_term)\n",
    "print(dropout)\n",
    "print(epochs_number)\n",
    "print(batch_size_number)\n",
    "print('ANN,R2',corr_ann)\n",
    "print('ANN,RMSE',rmse_val)\n",
    "print(y_real_maximum_power)\n",
    "###########visualization##########\n",
    "x_y_x=np.arange(0,100,10)\n",
    "x_y_y=np.arange(0,100,10)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(x_prediction_maximum_power_ann,y_real_maximum_power,color='red',label='Artificial Neural Network')\n",
    "plt.legend()\n",
    "ax.plot(x_y_x,x_y_y)\n",
    "plt.xlabel(u\"Predicted_Degradable_Conductivity %\")\n",
    "plt.ylabel(u\"Real_Degradable_Conductivity %\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745d7d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('ANN.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf21",
   "language": "python",
   "name": "tf21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
